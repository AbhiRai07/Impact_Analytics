{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e70e335",
   "metadata": {},
   "source": [
    "# Impact Analytics assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8115166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "import pickle\n",
    "import joblib\n",
    "import cv2\n",
    "import bz2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c44d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDUCE_MEM = True\n",
    "MODEL_FILE_DIR = 'imaterialist2020-pretrain-models/'\n",
    "attr_image_size = (160,160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af23780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_training = not os.path.isfile(MODEL_FILE_DIR+\"maskmodel_%d.model\"%attr_image_size[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55dbe1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"imaterialist-fashion-2020-fgvc7/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb5702fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ImageId', 'EncodedPixels', 'Height', 'Width', 'ClassId',\n",
       "       'AttributesIds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5477370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_to_mask(rle_string,height,width):\n",
    "    rows, cols = height, width\n",
    "    if rle_string == -1:\n",
    "        return np.zeros((height, width))\n",
    "    else:\n",
    "        rleNumbers = [int(numstring) for numstring in rle_string.split(' ')]\n",
    "        rlePairs = np.array(rleNumbers).reshape(-1,2)\n",
    "        img = np.zeros(rows*cols,dtype=np.uint8)\n",
    "        for index,length in rlePairs:\n",
    "            index -= 1\n",
    "            img[index:index+length] = 255\n",
    "        img = img.reshape(cols,rows)\n",
    "        img = img.T\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80822f0",
   "metadata": {},
   "source": [
    "# Find the included attribute ID for each class ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9747533",
   "metadata": {},
   "source": [
    "Getting the max class id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f2adc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "max_clz = train_df.ClassId.max()\n",
    "print(max_clz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f66659f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115,136,142,146,225,295,316,317\n",
      "115\n",
      "<class 'str'>\n",
      "115\n",
      "<class 'int'>\n",
      "136\n",
      "<class 'str'>\n",
      "136\n",
      "<class 'int'>\n",
      "142\n",
      "<class 'str'>\n",
      "142\n",
      "<class 'int'>\n",
      "146\n",
      "<class 'str'>\n",
      "146\n",
      "<class 'int'>\n",
      "225\n",
      "<class 'str'>\n",
      "225\n",
      "<class 'int'>\n",
      "295\n",
      "<class 'str'>\n",
      "295\n",
      "<class 'int'>\n",
      "316\n",
      "<class 'str'>\n",
      "316\n",
      "<class 'int'>\n",
      "317\n",
      "<class 'str'>\n",
      "317\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creating step by step process, Remove comment from below code and run\n",
    "\"\"\"\n",
    "\"\"\"max_attr = 0\n",
    "b = train_df.AttributesIds[1]\n",
    "print(b)\n",
    "for a in str(b).split(','):\n",
    "    print(a)\n",
    "    print(type(a))\n",
    "    if a != 'nan':\n",
    "        a = int(a)\n",
    "        print(a)\n",
    "        print(type(a))\n",
    "        if a > max_attr:\n",
    "            max_attr = a\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe552a6a",
   "metadata": {},
   "source": [
    "Reading the Attributes id one by one, then in each attribute id, splitting all the value and converting them to integer to compare and get max attribute id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00340ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_attr = 0\n",
    "for i in train_df.AttributesIds:\n",
    "    for a in str(i).split(','):\n",
    "        if a!='nan':\n",
    "            a = int(a)\n",
    "            if a > max_attr:\n",
    "                max_attr = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee64f22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340\n"
     ]
    }
   ],
   "source": [
    "print(max_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734358b",
   "metadata": {},
   "source": [
    "# Flag of whether or not there is an attribute ID for each class ID is a two-dimensional array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96e81f0",
   "metadata": {},
   "source": [
    "Adding 1 in both max_clz and max_attr because both of them starts from 0 and goes till max value. So total size will be added by 1 in order to include 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9532f9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 341)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clz_attr = np.zeros((max_clz+1,max_attr+1))\n",
    "clz_attrid2idx = [[] for _ in range(max_clz+1)]\n",
    "clz_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cae45c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c,i in zip(train_df.ClassId,train_df.AttributesIds):\n",
    "    for a in str(i).split(','):\n",
    "        if a!='nan':\n",
    "            a = int(a)\n",
    "            clz_attr[c,a] = 1\n",
    "            if not a in clz_attrid2idx[c]:\n",
    "                clz_attrid2idx[c].append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aadc139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[115, 136, 142, 146, 225, 295, 316, 317, 135, 145, 147, 328, 304, 138, 298, 301, 320, 322, 137, 325, 148, 229, 318, 141, 302, 340, 114, 149, 327, 234, 305, 323, 140, 228, 338, 333, 296, 335, 227, 326, 313, 314, 319, 281, 336, 307, 134, 290, 331, 312, 321, 283, 116, 332, 306, 151, 128, 324, 297, 315, 339, 308, 162, 163, 150, 160, 204, 309, 153, 286, 330, 303, 310, 139, 337, 232, 143, 226, 231, 329, 300, 233, 126, 334], [0, 115, 145, 146, 295, 316, 317, 11, 135, 136, 314, 147, 301, 320, 141, 340, 149, 332, 114, 142, 319, 313, 14, 137, 318, 328, 9, 312, 325, 326, 281, 322, 2, 10, 308, 1, 13, 144, 12, 116, 8, 7, 305, 282, 285, 324, 4, 16, 148, 138, 140, 5, 304, 321, 331, 3, 283, 15, 327, 323, 6, 302, 307, 339, 139, 298, 310, 330, 160, 204, 296, 198, 299, 120, 315, 334, 333, 297, 306, 337, 290, 179, 338, 335, 182, 181, 293, 286, 309, 329, 311, 289, 150, 152, 183, 151, 134, 153, 155, 292, 154, 303, 159, 336, 133, 185, 300, 143, 197, 199], [136, 147, 295, 316, 317, 115, 142, 135, 145, 146, 328, 141, 310, 149, 301, 324, 330, 137, 318, 281, 313, 332, 114, 148, 329, 312, 325, 320, 321, 323, 138, 289, 319, 333, 326, 308, 331, 283, 315, 322, 286, 297, 139, 150, 309, 140, 302, 335, 134, 337, 314, 305, 151, 340, 299, 160, 304, 327, 282], [115, 136, 145, 147, 225, 295, 316, 317, 146, 281, 312, 319, 137, 142, 149, 135, 301, 326, 332, 228, 153, 148, 318, 335, 229, 152, 138, 311, 328, 323, 234, 324, 283, 150, 141, 226, 151, 227, 340, 114, 305, 331, 231, 313, 302, 232, 315, 289, 322, 327, 333, 321, 309, 314, 310, 325, 330, 297, 154, 300, 233, 140, 286, 290, 160, 204, 307], [17, 115, 136, 145, 149, 225, 295, 311, 317, 135, 142, 146, 308, 316, 328, 28, 116, 147, 229, 283, 313, 32, 305, 21, 298, 19, 289, 303, 22, 137, 321, 141, 227, 226, 281, 148, 307, 33, 114, 20, 296, 30, 314, 34, 312, 301, 29, 322, 325, 315, 228, 18, 286, 138, 326, 26, 35, 324, 24, 27, 31, 323, 299, 318, 234, 291, 319, 231, 290, 297, 150, 232, 300, 302, 340, 292, 334, 309, 310, 282, 327, 339, 23, 143, 140, 333, 160, 204, 233, 329, 285, 304, 320, 152, 331, 293, 330, 335, 337, 332, 132, 131, 25, 133, 338, 126, 151], [115, 136, 145, 146, 229, 295, 298, 317, 135, 142, 225, 281, 311, 137, 147, 289, 316, 234, 301, 332, 297, 283, 314, 322, 306, 149, 290, 308, 339, 312, 303, 323, 141, 228, 226, 315, 318, 148, 328, 140, 331, 319, 330, 114, 227, 232, 138, 134, 325, 304, 326, 291, 296, 310, 151, 300, 329, 282, 233, 293, 292, 334, 143, 320, 321, 327, 305, 150, 299], [115, 136, 143, 154, 230, 295, 316, 317, 137, 141, 234, 305, 135, 125, 326, 38, 134, 142, 301, 36, 297, 298, 128, 41, 132, 229, 45, 283, 155, 46, 329, 126, 49, 319, 44, 131, 304, 37, 153, 312, 325, 39, 328, 320, 318, 322, 42, 127, 306, 225, 300, 331, 152, 282, 47, 340, 43, 138, 327, 120, 124, 309, 313, 337, 40, 314, 281, 315, 323, 321, 296, 226, 338, 324, 339, 117, 48, 311, 149, 335, 333, 118, 145, 332, 310, 330, 302, 286, 227, 303, 289, 151, 308, 114, 150, 299, 334, 290, 146, 222, 133, 307], [50, 135, 142, 149, 295, 301, 325, 115, 136, 148, 230, 300, 317, 54, 227, 52, 151, 322, 143, 316, 298, 137, 150, 229, 304, 60, 225, 283, 234, 305, 61, 131, 57, 59, 141, 306, 138, 314, 310, 321, 323, 328, 297, 58, 312, 302, 134, 319, 320, 318, 53, 333, 56, 152, 55, 326, 281, 286, 313, 226, 232, 282, 335, 315, 153, 51, 331, 296, 293, 340, 299, 324, 114, 290, 339, 289, 337, 332, 334, 231, 327, 329, 140, 228, 132, 308, 128, 309, 307], [67, 127, 142, 150, 229, 309, 317, 118, 152, 295, 301, 115, 133, 154, 316, 325, 68, 128, 149, 71, 304, 151, 141, 328, 114, 129, 120, 143, 225, 305, 326, 65, 298, 153, 69, 155, 234, 77, 117, 130, 123, 148, 230, 300, 321, 318, 122, 282, 72, 322, 281, 323, 62, 64, 315, 70, 334, 319, 340, 126, 320, 78, 308, 335, 302, 66, 313, 119, 290, 74, 307, 226, 145, 338, 312, 327, 228, 75, 283, 314, 331, 332, 285, 336, 76, 121, 333, 297, 296, 337, 329, 227, 63, 124, 292, 231, 116, 339, 324, 232, 144, 293, 306, 310, 136, 286, 137, 73, 289, 303, 299, 330, 140, 135, 132, 138, 311, 156, 294], [136, 149, 225, 311, 317, 82, 115, 142, 226, 295, 314, 332, 81, 145, 79, 137, 233, 289, 128, 92, 135, 150, 228, 316, 326, 86, 155, 152, 313, 90, 116, 151, 301, 325, 322, 146, 94, 91, 85, 303, 141, 129, 229, 88, 340, 147, 84, 318, 138, 328, 324, 302, 310, 148, 140, 321, 153, 87, 281, 114, 312, 335, 330, 80, 320, 333, 319, 296, 329, 154, 298, 93, 83, 305, 291, 292, 327, 309, 315, 119, 290, 282, 227, 304, 232, 331, 299, 338, 283, 89, 143, 306, 308, 285, 323, 231, 334, 127, 307, 337, 294, 234, 339, 286, 300], [102, 128, 142, 150, 229, 295, 301, 318, 108, 115, 119, 141, 155, 286, 316, 317, 305, 104, 129, 145, 149, 308, 325, 112, 114, 151, 311, 322, 302, 123, 281, 307, 313, 312, 97, 153, 234, 95, 127, 103, 147, 225, 133, 227, 309, 106, 326, 101, 130, 304, 122, 120, 314, 315, 113, 117, 118, 339, 109, 328, 105, 306, 283, 152, 154, 327, 340, 323, 140, 303, 296, 98, 121, 148, 289, 143, 116, 111, 319, 107, 139, 100, 282, 144, 138, 226, 320, 96, 228, 329, 126, 333, 310, 137, 285, 331, 110, 337, 332, 99, 136, 300, 146, 335, 330, 336, 232, 298, 324, 231, 135, 334, 297, 156, 299, 321, 287, 160, 204, 338, 230, 233, 292, 290, 294, 288, 159, 293], [115, 125, 141, 155, 295, 301, 325, 126, 142, 154, 229, 305, 306, 317, 316, 135, 234, 137, 149, 318, 335, 337, 225, 304, 136, 143, 309, 298, 326, 328, 145, 132, 319, 114, 281, 323, 128, 311, 228, 302, 312, 336, 313, 314, 315, 340, 230, 299, 297, 296, 331, 148, 339, 153, 150, 227, 131, 152, 324, 332, 151, 327, 322, 320, 308, 232, 129, 333, 321, 307, 138, 226, 334, 120, 338, 116, 283, 300, 139, 124, 147, 117, 127, 119, 310, 122], [115, 137, 145, 150, 295, 305, 317, 136, 146, 312, 114, 138, 225, 311, 147, 316, 142, 149, 234, 325, 301, 322, 151, 324, 300, 289, 319, 148, 227, 231, 281, 229, 296, 318, 323, 328, 306, 141, 152, 155, 313, 226, 153, 320, 228, 232, 291, 326, 327, 310, 290, 329, 308, 321, 340, 133, 154, 283], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [283, 317, 16, 115], [163, 166, 169, 162, 168, 167, 165, 173, 170, 164, 161, 172, 171, 115, 136, 295, 316, 317, 135, 146, 225], [174, 175, 176, 178, 177], [], [160, 204, 159, 206, 157, 207, 214, 209, 213, 205, 210, 158, 216, 212, 215, 211, 156, 208, 149, 316, 0, 115, 135, 146, 295, 317, 305], [219, 218, 224, 222, 223, 221, 220, 217], [190, 182, 187, 188, 197, 183, 192, 194, 200, 198, 186, 181, 185, 179, 184, 203, 189, 195, 180, 202, 191, 201, 193, 196, 199, 2, 115, 145, 0, 135, 146, 316, 317, 13, 11, 295, 136, 301], [], [219, 222, 221, 218, 223], [182], [190, 183, 200, 182, 218, 192, 203], [], [199], [190], [], [], [160, 210, 115, 295, 316, 317, 180, 187, 156, 200, 158, 209, 211], [], []]\n"
     ]
    }
   ],
   "source": [
    "print(clz_attrid2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c6fa24",
   "metadata": {},
   "source": [
    "# Indicates the number of attribute IDs for each class ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac6138b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 84, 110,  59,  67, 107,  69, 102,  89, 119, 105, 128,  86,  58,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   4,  21,   5,   0,  27,   8,  38,   0,   5,   1,   7,   0,\n",
       "         1,   1,   0,   0,  13,   0,   0], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clz_attr_num = clz_attr.sum(axis=1).astype(np.int64)\n",
    "clz_attr_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e32f4d",
   "metadata": {},
   "source": [
    "You can see that some class IDs do not have attribute IDs associated with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5571d1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>ClassId</th>\n",
       "      <th>AttributesIds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000663ed1ff0c4e0132b9b9ac53f6e</td>\n",
       "      <td>6068157 7 6073371 20 6078584 34 6083797 48 608...</td>\n",
       "      <td>5214</td>\n",
       "      <td>3676</td>\n",
       "      <td>6</td>\n",
       "      <td>115,136,143,154,230,295,316,317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000663ed1ff0c4e0132b9b9ac53f6e</td>\n",
       "      <td>6323163 11 6328356 32 6333549 53 6338742 75 63...</td>\n",
       "      <td>5214</td>\n",
       "      <td>3676</td>\n",
       "      <td>0</td>\n",
       "      <td>115,136,142,146,225,295,316,317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000663ed1ff0c4e0132b9b9ac53f6e</td>\n",
       "      <td>8521389 10 8526585 30 8531789 42 8537002 46 85...</td>\n",
       "      <td>5214</td>\n",
       "      <td>3676</td>\n",
       "      <td>28</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000663ed1ff0c4e0132b9b9ac53f6e</td>\n",
       "      <td>12903854 2 12909064 7 12914275 10 12919485 15 ...</td>\n",
       "      <td>5214</td>\n",
       "      <td>3676</td>\n",
       "      <td>31</td>\n",
       "      <td>160,204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000663ed1ff0c4e0132b9b9ac53f6e</td>\n",
       "      <td>10837337 5 10842542 14 10847746 24 10852951 33...</td>\n",
       "      <td>5214</td>\n",
       "      <td>3676</td>\n",
       "      <td>32</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333396</th>\n",
       "      <td>fffe20b555b98c3c1f26c8dfff275cbc</td>\n",
       "      <td>2712731 8 2715725 23 2718719 39 2721713 55 272...</td>\n",
       "      <td>3000</td>\n",
       "      <td>2001</td>\n",
       "      <td>28</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333397</th>\n",
       "      <td>ffffbf7014a9e408bfbb81a75bc70638</td>\n",
       "      <td>71179 1 71678 3 72178 4 72678 4 73178 5 73679 ...</td>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333398</th>\n",
       "      <td>ffffbf7014a9e408bfbb81a75bc70638</td>\n",
       "      <td>116648 5 117148 16 117648 22 118148 26 118647 ...</td>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>31</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333399</th>\n",
       "      <td>ffffbf7014a9e408bfbb81a75bc70638</td>\n",
       "      <td>67711 1 68210 1 68709 2 69204 2 69208 3 69705 ...</td>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>31</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333400</th>\n",
       "      <td>ffffbf7014a9e408bfbb81a75bc70638</td>\n",
       "      <td>63365 36 63852 77 64343 85 64838 89 65332 94 6...</td>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>10</td>\n",
       "      <td>102,128,142,150,295,308,317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333401 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ImageId  \\\n",
       "0       00000663ed1ff0c4e0132b9b9ac53f6e   \n",
       "1       00000663ed1ff0c4e0132b9b9ac53f6e   \n",
       "2       00000663ed1ff0c4e0132b9b9ac53f6e   \n",
       "3       00000663ed1ff0c4e0132b9b9ac53f6e   \n",
       "4       00000663ed1ff0c4e0132b9b9ac53f6e   \n",
       "...                                  ...   \n",
       "333396  fffe20b555b98c3c1f26c8dfff275cbc   \n",
       "333397  ffffbf7014a9e408bfbb81a75bc70638   \n",
       "333398  ffffbf7014a9e408bfbb81a75bc70638   \n",
       "333399  ffffbf7014a9e408bfbb81a75bc70638   \n",
       "333400  ffffbf7014a9e408bfbb81a75bc70638   \n",
       "\n",
       "                                            EncodedPixels  Height  Width  \\\n",
       "0       6068157 7 6073371 20 6078584 34 6083797 48 608...    5214   3676   \n",
       "1       6323163 11 6328356 32 6333549 53 6338742 75 63...    5214   3676   \n",
       "2       8521389 10 8526585 30 8531789 42 8537002 46 85...    5214   3676   \n",
       "3       12903854 2 12909064 7 12914275 10 12919485 15 ...    5214   3676   \n",
       "4       10837337 5 10842542 14 10847746 24 10852951 33...    5214   3676   \n",
       "...                                                   ...     ...    ...   \n",
       "333396  2712731 8 2715725 23 2718719 39 2721713 55 272...    3000   2001   \n",
       "333397  71179 1 71678 3 72178 4 72678 4 73178 5 73679 ...     500    375   \n",
       "333398  116648 5 117148 16 117648 22 118148 26 118647 ...     500    375   \n",
       "333399  67711 1 68210 1 68709 2 69204 2 69208 3 69705 ...     500    375   \n",
       "333400  63365 36 63852 77 64343 85 64838 89 65332 94 6...     500    375   \n",
       "\n",
       "        ClassId                    AttributesIds  \n",
       "0             6  115,136,143,154,230,295,316,317  \n",
       "1             0  115,136,142,146,225,295,316,317  \n",
       "2            28                              163  \n",
       "3            31                          160,204  \n",
       "4            32                              219  \n",
       "...         ...                              ...  \n",
       "333396       28                              163  \n",
       "333397       33                              NaN  \n",
       "333398       31                              157  \n",
       "333399       31                              157  \n",
       "333400       10      102,128,142,150,295,308,317  \n",
       "\n",
       "[333401 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09a3d4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "['ffffbf7014a9e408bfbb81a75bc70638'\n",
      " '63365 36 63852 77 64343 85 64838 89 65332 94 65827 99 66321 106 66816 112 67300 129 67694 5 67711 1 67785 145 68185 15 68210 1 68281 149 68684 18 68709 2 68777 152 69183 22 69208 3 69273 156 69682 30 69769 160 70181 31 70230 12 70266 164 70680 32 70718 27 70762 169 71179 34 71215 33 71258 174 71678 73 71755 178 72178 255 72678 254 73178 254 73679 254 74179 256 74679 257 75179 257 75679 257 76179 256 76679 256 77179 256 77679 256 78179 256 78679 257 79179 258 79679 259 80178 260 80678 100 80781 157 81178 98 81281 157 81678 97 81781 157 82178 97 82281 157 82678 96 82785 153 83178 96 83286 152 83678 90 83770 3 83786 152 84178 89 84271 2 84285 154 84678 88 84771 2 84785 5 84792 148 85178 88 85271 1 85285 4 85292 149 85678 87 85771 1 85785 3 85792 149 86177 88 86284 4 86293 147 86677 87 86784 4 86793 146 87177 87 87284 3 87293 145 87676 87 87784 3 87793 145 88176 87 88284 3 88293 145 88676 87 88783 3 88793 145 89175 87 89283 3 89293 145 89675 87 89783 3 89793 145 90175 86 90283 3 90293 145 90674 87 90783 3 90793 145 91174 87 91283 3 91293 145 91674 86 91783 3 91793 145 92174 86 92282 4 92293 145 92673 87 92782 4 92793 145 93173 87 93282 4 93293 145 93673 87 93782 4 93793 145 94172 89 94282 4 94293 145 94672 89 94784 2 94793 146 95172 89 95293 147 95671 90 95793 148 96171 90 96293 149 96671 91 96793 150 97170 92 97293 150 97669 93 97794 148 98168 94 98294 148 98667 95 98794 147 99167 96 99295 146 99666 97 99795 146 100165 98 100295 145 100664 99 100796 145 101164 100 101296 146 101663 101 101796 147 102162 102 102297 147 102661 104 102797 146 103160 105 103297 146 103659 106 103798 144 104158 108 104298 144 104656 110 104798 144 105155 111 105298 143 105654 113 105798 143 106152 113 106298 142 106651 111 106798 142 107150 111 107299 142 107646 113 107799 142 108139 119 108299 143 108632 125 108799 142 109128 127 109299 141 109626 127 109799 140 110123 128 110299 139 110621 129 110799 138 111119 129 111299 138 111617 129 111799 137 112116 128 112299 137 112616 127 112799 136 113117 124 113299 135 113617 122 113799 134 114117 120 114298 134 114617 19 114640 95 114798 108 114920 11 115117 16 115144 89 115298 104 115618 13 115645 86 115797 100 116118 12 116146 83 116300 92 116618 10 116647 80 116805 82 117118 9 117148 77 117311 71 117619 6 117648 75 117816 61 118119 5 118148 73 118320 52 118619 4 118647 72 118824 43 119120 2 119147 70 119330 29 119620 2 119647 68 119839 9 120120 1 120147 66 120647 64 121147 62 121647 60 122146 59 122646 57 123146 55 123646 53 124146 51 124645 50 125145 48 125645 45 126144 45 126644 46 127144 47 127643 49 128143 50 128643 51 129142 53 129642 54 130142 55 130641 57 131141 58 131640 60 132140 61 132641 61 133142 61 133643 61 134144 59 134646 57 135147 55 135648 54 136150 51 136651 49 137152 48 137653 46 138155 44 138656 42 139157 41 139659 38 140160 37 140661 35 141162 34 141664 31 142165 29 142666 27 143168 25 143669 23 144170 21 144672 18 145173 17 145675 14 146176 12 146678 9 147179 8 147680 6 148182 3 148683 1'\n",
      " 500 375 10 '102,128,142,150,295,308,317']\n",
      "['102', '128', '142', '150', '295', '308', '317']\n",
      "Attribute\n",
      "<map object at 0x00000182803A95C8>\n",
      "where\n",
      "(array([115, 115, 116, ..., 441, 441, 442], dtype=int64), array([224, 225, 223, ..., 205, 206, 204], dtype=int64))\n",
      "(500, 375)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'res = []\\nres.append((cid, mask, attr, X))\\nAfter coming out of loop contains all classes, mask resized, attributes, roi x resized\\n__getitem_ return res, resized image and image id'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Creating step by step process. Remove multiline comment from below code and run to get the given output\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"xe = train_df[train_df[\"ImageId\"]==\"ffffbf7014a9e408bfbb81a75bc70638\"]\n",
    "print(len(xe))\n",
    "t=xe.values[3]\n",
    "print(t)\n",
    "class_id = t[4]\n",
    "mask = rle_to_mask(t[1],t[2],t[3])    #rle_string,height,width\n",
    "#xe=0\n",
    "plt.imshow(mask)\n",
    "attr = str(t[5]).split(\",\")\n",
    "print(attr)\n",
    "attr = map(int,str(t[5]).split(\",\"))\n",
    "print(\"Attribute\")\n",
    "print(attr)\n",
    "where = np.where(mask != 0)\n",
    "print(\"where\")\n",
    "print(where)\n",
    "print(mask.shape)\n",
    "xe = 0\"\"\"\n",
    "\n",
    "#res = []\n",
    "#res.append((cid, mask, attr, X))\n",
    "#After coming out of loop contains all classes, mask resized, attributes, roi x resized\n",
    "#__getitem_ return res, resized image and image id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bf51d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptoz(obj):\n",
    "    return bz2.compress(pickle.dumps(obj), 3) if REDUCE_MEM else obj\n",
    "def ztop(b):\n",
    "    return pickle.loads(bz2.decompress(b)) if REDUCE_MEM else b\n",
    "def __getitem__(imgid):\n",
    "    df = train_df[train_df.ImageId==imgid]\n",
    "    res = []\n",
    "    imag = cv2.imread(\"imaterialist-fashion-2020-fgvc7/train/\"+str(imgid)+\".jpg\")\n",
    "    for idx in range(len(df)):\n",
    "        t = df.values[idx]\n",
    "        cid = t[4]\n",
    "        mask = rle_to_mask(t[1],t[2],t[3])\n",
    "        attr = map(int,str(t[5]).split(\",\")) if str(t[5]) != 'nan' else []\n",
    "        where = np.where(mask != 0)\n",
    "        y1,y2,x1,x2 = 0,0,0,0\n",
    "        if len(where[0]) > 0 and len(where[1]) > 0:\n",
    "            y1,y2,x1,x2 = min(where[0]),max(where[0]),min(where[1]),max(where[1])\n",
    "        if y2>y1+10 and x2>x1+10:\n",
    "            X = cv2.resize(imag[y1:y2,x1:x2], attr_image_size)\n",
    "            X = ptoz(X)\n",
    "        else:\n",
    "            X = None\n",
    "        mask = cv2.resize(mask, attr_image_size)\n",
    "        mask = ptoz(mask)\n",
    "        res.append((cid, mask, attr, X))\n",
    "    imag = cv2.resize(imag, attr_image_size)\n",
    "    imag = ptoz(imag)\n",
    "    return res, imag, imgid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4c698b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1735ff992a447bbcc323612bab9e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9408\\72625460.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mdata_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageId\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[0mdata_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9408\\1395903822.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(imgid)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my2\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m10\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_image_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if to_training:\n",
    "    if os.path.isfile(MODEL_FILE_DIR+\"data_cache_%d\"%attr_image_size[0]):\n",
    "        data_cache = joblib.load(MODEL_FILE_DIR+\"data_cache_%d\"%attr_image_size[0])\n",
    "    elif REDUCE_MEM:\n",
    "        data_cache = []\n",
    "        for i in tqdm(list(set(train_df.ImageId))):\n",
    "            res, imag, imgid = __getitem__(i)\n",
    "            for cid, mask, attr, X in res:\n",
    "                data_cache.append((cid, mask, attr, imag, X, imgid))\n",
    "        joblib.dump(data_cache, MODEL_FILE_DIR+\"data_cache_%d\"%attr_image_size[0])\n",
    "    else:\n",
    "        with Pool(8) as p:\n",
    "            tmp = p.map(__getitem__, list(set(train_df.ImageId)))\n",
    "        data_cache = []\n",
    "        for res, imag, imgid in tmp:\n",
    "            for cid, mask, attr, X in res:\n",
    "                data_cache.append((cid, mask, attr, imag, X, imgid))\n",
    "        del tmp\n",
    "        joblib.dump(data_cache, MODEL_FILE_DIR+\"data_cache_%d\"%attr_image_size[0])\n",
    "else:\n",
    "    data_cache = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3d4764",
   "metadata": {},
   "source": [
    "Interrupted the kernel manually so that can show approach but real time training will take a lot of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "667ffc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.jit.script\n",
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return mish(input)\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n",
    "        super(SeparableConv2d,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,in_filters,out_filters,reps,strides=1,activation=None):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        if out_filters != in_filters or strides!=1:\n",
    "            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n",
    "            self.skipbn = nn.BatchNorm2d(out_filters)\n",
    "        else:\n",
    "            self.skip=None\n",
    "\n",
    "        act = nn.ReLU() if activation is None else activation\n",
    "        rep=[]\n",
    "\n",
    "        rep.append(act)\n",
    "        rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
    "        rep.append(nn.BatchNorm2d(out_filters))\n",
    "        filters = out_filters\n",
    "\n",
    "        for i in range(reps-1):\n",
    "            rep.append(act)\n",
    "            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(filters))\n",
    "\n",
    "        if strides != 1:\n",
    "            rep.append(nn.MaxPool2d(3,strides,1))\n",
    "        self.rep = nn.Sequential(*rep)\n",
    "\n",
    "    def forward(self,inp):\n",
    "        x = self.rep(inp)\n",
    "\n",
    "        if self.skip is not None:\n",
    "            skip = self.skip(inp)\n",
    "            skip = self.skipbn(skip)\n",
    "        else:\n",
    "            skip = inp\n",
    "\n",
    "        x += skip\n",
    "        return x\n",
    "\n",
    "class AttrXception(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AttrXception, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, 2, 1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.mish = Mish()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, 1, 1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.block1 = Block(128,256,2,2)\n",
    "        self.block2 = Block(256,256,3,1)\n",
    "        self.block3 = Block(256,256,3,1)\n",
    "        self.block4 = Block(256,256,3,1)\n",
    "        self.block5 = Block(256,256,3,1)\n",
    "        self.block6 = Block(256,256,3,1)\n",
    "        self.block7 = Block(256,384,2,2)\n",
    "\n",
    "        self.conv3 = SeparableConv2d(384,512,3,stride=1,padding=0,bias=True)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = self.mish(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "\n",
    "        x = self.mish(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = self.mish(x)\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        result = self.fc(x)\n",
    "        \n",
    "        return torch.sigmoid(result)\n",
    "    \n",
    "class HourglassNet(nn.Module):\n",
    "    def __init__(self, depth, channel):\n",
    "        super(HourglassNet, self).__init__()\n",
    "        self.depth = depth\n",
    "        hg = []\n",
    "        for _ in range(self.depth):\n",
    "            hg.append([\n",
    "                Block(channel,channel,3,1,activation=Mish()),\n",
    "                Block(channel,channel,2,2,activation=Mish()),\n",
    "                Block(channel,channel,3,1,activation=Mish())\n",
    "            ])\n",
    "        hg[0].append(Block(channel,channel,3,1,activation=Mish()))\n",
    "        hg = [nn.ModuleList(h) for h in hg]\n",
    "        self.hg = nn.ModuleList(hg)\n",
    "\n",
    "    def _hour_glass_forward(self, n, x):\n",
    "        up1 = self.hg[n-1][0](x)\n",
    "        low1 = self.hg[n-1][1](up1)\n",
    "\n",
    "        if n > 1:\n",
    "            low2 = self._hour_glass_forward(n-1, low1)\n",
    "        else:\n",
    "            low2 = self.hg[n-1][3](low1)\n",
    "\n",
    "        low3 = self.hg[n-1][2](low2)\n",
    "        up2 = F.interpolate(low3, scale_factor=2)\n",
    "        out = up1 + up2\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._hour_glass_forward(self.depth, x)\n",
    "\n",
    "class XceptionHourglass(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(XceptionHourglass, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 128, 3, 2, 1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.mish = Mish()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(128, 256, 3, 1, 1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.block1 = HourglassNet(4, 256)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.block2 = HourglassNet(4, 256)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(256, num_classes, 1, bias=True)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = self.mish(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.mish(x)\n",
    "\n",
    "        out1 = self.block1(x)\n",
    "        x = self.bn3(out1)\n",
    "        x = self.mish(x)\n",
    "        out2 = self.block2(x)\n",
    "\n",
    "        r = self.sigmoid(out1 + out2)\n",
    "        r = F.interpolate(r, scale_factor=2)\n",
    "        \n",
    "        return self.conv3(r)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "737f4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDataset(object):\n",
    "    def __init__(self, chaches, clzid):\n",
    "        self.clzid = clzid\n",
    "        self.chaches = [cd for cd in chaches if cd[0]==clzid]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cid, mask, attr, imag, X, imgid = self.chaches[idx]\n",
    "        mask = ztop(mask)\n",
    "        imag = ztop(imag)\n",
    "        if X is None:\n",
    "            X = imag\n",
    "        else:\n",
    "            X = ztop(X)\n",
    "        y = np.zeros(clz_attr_num[self.clzid])\n",
    "        for a in attr:\n",
    "            y[clz_attrid2idx[self.clzid].index(a)] = 1\n",
    "        return X.transpose((2,0,1)).astype(np.float32), y.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.chaches)\n",
    "\n",
    "def train_attr_net(clzid, num_epochs=1):\n",
    "    data = AttrDataset(data_cache, clzid)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        data, batch_size=64, shuffle=True, num_workers=1)\n",
    "\n",
    "    model = AttrXception(clz_attr_num[clzid])\n",
    "    model.cuda()\n",
    "    dp = torch.nn.DataParallel(model)\n",
    "    loss = nn.BCELoss()\n",
    "\n",
    "    params = [p for p in dp.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.RMSprop(params, lr=2.5e-4,  momentum=0.9)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=6,\n",
    "                                                   gamma=0.9)\n",
    "    \n",
    "    prog = tqdm(list(range(num_epochs)))\n",
    "    for epoch in prog:\n",
    "        for i, (X, y) in enumerate(data_loader):\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "            xx = dp(X)\n",
    "\n",
    "            losses = loss(xx, y)\n",
    "\n",
    "            prog.set_description(\"loss:%05f\"%losses)\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        X, xx, y, losses = None, None, None, None\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a4a50",
   "metadata": {},
   "source": [
    "# Training Attribute classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b23031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for clzid in range(len(clz_attr_num)):\n",
    "    if clz_attr_num[clzid] > 0:\n",
    "        if not os.path.isfile(MODEL_FILE_DIR+\"attrmodel_%d-%d.model\"%(attr_image_size[0],clzid)):\n",
    "            model = train_attr_net(clzid, 32)\n",
    "            torch.save(model.state_dict(), MODEL_FILE_DIR+\"attrmodel_%d-%d.model\"%(attr_image_size[0],clzid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da64ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e0f3b",
   "metadata": {},
   "source": [
    "Currently gpu not there so not trained\n",
    "\n",
    "But if trained the trained model was saved to a file for later use after the above scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c095bb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\envs\\nd\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "data_mask = dict()\n",
    "while len(data_cache) > 0:\n",
    "    cid, mask, _, imag, _, imgid = data_cache.pop()\n",
    "    mask = ztop(mask)\n",
    "    if imgid not in data_mask:\n",
    "        imag = ztop(imag)\n",
    "        data_mask[imgid] = [ptoz(imag.transpose((2,0,1)).astype(np.float32)), np.zeros(attr_image_size, dtype=np.int)]\n",
    "    data_mask[imgid][1][mask!=0] = cid + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2765e78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data_cache\n",
    "for k in data_mask.keys():\n",
    "    data_mask[k][1] = ptoz(data_mask[k][1])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe86731",
   "metadata": {},
   "source": [
    "# Next, train the mask image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfb31ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDataset(object):\n",
    "    def __init__(self, keys):\n",
    "        self.keys = keys\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        k = self.keys[idx]\n",
    "        return ztop(data_mask[k][0]), ztop(data_mask[k][1])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6dfb271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mask_net(num_epochs=1):\n",
    "    data = MaskDataset(list(data_mask.keys()))\n",
    "    data_loader = torch.utils.data.DataLoader(data, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "    model = XceptionHourglass(max_clz+2)\n",
    "    model.cuda()\n",
    "    dp = torch.nn.DataParallel(model)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    params = [p for p in dp.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.RMSprop(params, lr=2.5e-4,  momentum=0.9)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=6,\n",
    "                                                   gamma=0.9)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = []\n",
    "        prog = tqdm(data_loader, total=len(data_loader))\n",
    "        for i, (imag, mask) in enumerate(prog):\n",
    "            X = imag.cuda()\n",
    "            y = mask.cuda()\n",
    "            xx = dp(X)\n",
    "            # to 1D-array\n",
    "            y = y.reshape((y.size(0),-1))  # batch, flatten-img\n",
    "            y = y.reshape((y.size(0) * y.size(1),))  # flatten-all\n",
    "            xx = xx.reshape((xx.size(0), xx.size(1), -1))  # batch, channel, flatten-img\n",
    "            xx = torch.transpose(xx, 2, 1)  # batch, flatten-img, channel\n",
    "            xx = xx.reshape((xx.size(0) * xx.size(1),-1))  # flatten-all, channel\n",
    "\n",
    "            losses = loss(xx, y)\n",
    "\n",
    "            prog.set_description(\"loss:%05f\"%losses)\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss.append(losses.detach().cpu().numpy())\n",
    "            break\n",
    "\n",
    "        prog, X, xx, y, losses = None, None, None, None, None,\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e8f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "if to_training:\n",
    "    model = train_mask_net(64)\n",
    "    torch.save(model.state_dict(), MODEL_FILE_DIR+\"maskmodel_%d.model\"%attr_image_size[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8a76ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6569"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data_mask\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dadcbb",
   "metadata": {},
   "source": [
    "# Predict Mask Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "173b37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDataset(object):\n",
    "    def __init__(self, folder):\n",
    "        self.imgids = [f.split(\".\")[0] for f in os.listdir(folder)]\n",
    "        self.folder = folder\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imag = cv2.imread(self.folder+self.imgids[idx]+\".jpg\")\n",
    "        imag = cv2.resize(imag, attr_image_size)\n",
    "        return imag.transpose((2,0,1)).astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3167ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XceptionHourglass(max_clz+2)\n",
    "model.cuda()\n",
    "model.load_state_dict(torch.load(MODEL_FILE_DIR+\"maskmodel_%d.model\"%attr_image_size[0]))\n",
    "\n",
    "dataset = MaskDataset(\"..imaterialist-fashion-2020-fgvc7/test/\")\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "predict_imgeid = []\n",
    "predict_mask = []\n",
    "predict_rle = []\n",
    "predict_classid = []\n",
    "predict_attr = []\n",
    "\n",
    "model.eval()\n",
    "prog = tqdm(data_loader, total=len(data_loader))\n",
    "num_pred = 0\n",
    "for X in prog:\n",
    "    X = X.cuda()\n",
    "    pred = model(X).detach().cpu().numpy()\n",
    "    for i, mask in enumerate(pred):\n",
    "        imgid = dataset.imgids[num_pred]\n",
    "        num_pred += 1\n",
    "        pred_id = mask.argmax(axis=0) - 1  # -1 is background.\n",
    "        for clz in set(pred_id.reshape((-1,)).tolist()):\n",
    "            if clz >= 0:\n",
    "                maskdata = (pred_id == clz).astype(np.uint8) * 255\n",
    "                predict_imgeid.append(imgid)\n",
    "                predict_mask.append(maskdata)\n",
    "                predict_rle.append(\"\")\n",
    "                predict_classid.append(clz)\n",
    "                predict_attr.append([])\n",
    "prog, X, pred, dataset, data_loader = None, None, None, None, None\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be418b9c",
   "metadata": {},
   "source": [
    "# After generating all mask images, cut out only the mask image part and put it in the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8936f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def _scale_image(img, long_size):\n",
    "    if img.shape[0] < img.shape[1]:\n",
    "        scale = img.shape[1] / long_size\n",
    "        size = (long_size, math.floor(img.shape[0] / scale))\n",
    "    else:\n",
    "        scale = img.shape[0] / long_size\n",
    "        size = (math.floor(img.shape[1] / scale), long_size)\n",
    "    return cv2.resize(img, size, interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f74f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "for clzid in range(len(clz_attr_num)):\n",
    "    if clz_attr_num[clzid] > 0 and os.path.isfile(MODEL_FILE_DIR+\"attrmodel_%d-%d.model\"%(attr_image_size[0],clzid)):\n",
    "        model = AttrXception(clz_attr_num[clzid])\n",
    "        model.cuda()\n",
    "        model.eval()\n",
    "        model.load_state_dict(torch.load(MODEL_FILE_DIR+\"attrmodel_%d-%d.model\"%(attr_image_size[0],clzid)))\n",
    "        for i in range(len(predict_classid)):\n",
    "            if predict_classid[i] == clzid:\n",
    "                imag = cv2.imread(\"imaterialist-fashion-2020-fgvc7/test/\"+predict_imgeid[i]+\".jpg\")\n",
    "                imag = _scale_image(imag, 1024)\n",
    "                mask = cv2.resize(predict_mask[i], (imag.shape[1],imag.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "                where = np.where(mask!=0)\n",
    "                y1,y2,x1,x2 = 0,0,0,0\n",
    "                if len(where[0]) > 0 and len(where[1]) > 0:\n",
    "                    y1,y2,x1,x2 = min(where[0]),max(where[0]),min(where[1]),max(where[1])\n",
    "                    if y2>y1+80 and x2>x1+80 and np.sum(mask)/255 > 1000:\n",
    "                        print(\"class id=\",clzid)\n",
    "                        plt.subplot(1,2,1)\n",
    "                        plt.imshow(imag)\n",
    "                        plt.subplot(1,2,2)\n",
    "                        plt.imshow(mask)\n",
    "                        plt.show()\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "uses_index = []\n",
    "for clzid in tqdm(range(len(clz_attr_num))):\n",
    "    if clz_attr_num[clzid] > 0 and os.path.isfile(MODEL_FILE_DIR+\"attrmodel_%d-%d.model\"%(attr_image_size[0],clzid)):\n",
    "        model = AttrXception(clz_attr_num[clzid])\n",
    "        model.cuda()\n",
    "        model.eval()\n",
    "        model.load_state_dict(torch.load(MODEL_FILE_DIR+\"attrmodel_%d-%d.model\"%(attr_image_size[0],clzid)))\n",
    "        for i in range(len(predict_classid)):\n",
    "            if predict_classid[i] == clzid:\n",
    "                imag = cv2.imread(\"imaterialist-fashion-2020-fgvc7/test/\"+predict_imgeid[i]+\".jpg\")\n",
    "                imag = _scale_image(imag, 1024)\n",
    "                mask = cv2.resize(predict_mask[i], (imag.shape[1],imag.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "                #imag[mask==0] = 255\n",
    "                where = np.where(mask!=0)\n",
    "                y1,y2,x1,x2 = 0,0,0,0\n",
    "                if len(where[0]) > 0 and len(where[1]) > 0:\n",
    "                    y1,y2,x1,x2 = min(where[0]),max(where[0]),min(where[1]),max(where[1])\n",
    "                    if y2>y1+80 and x2>x1+80 and np.sum(mask)/255 > 1000:\n",
    "                        predict_rle[i] = mask_to_rle(mask)\n",
    "                        X = cv2.resize(imag[y1:y2,x1:x2], attr_image_size).transpose((2,0,1))\n",
    "                        attr_preds = model(torch.tensor([X], dtype=torch.float32).cuda())\n",
    "                        attr_preds = attr_preds.detach().cpu().numpy()[0]\n",
    "                        for ci in range(len(attr_preds)):\n",
    "                            if attr_preds[ci] > 0.5:\n",
    "                                uses_index.append(i)\n",
    "                                predict_attr[i].append(clz_attrid2idx[predict_classid[i]][ci])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a36333",
   "metadata": {},
   "source": [
    "# If the threshold (0.5) is exceeded, it is assumed that the attribute ID is included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_attri_str = [\",\".join(list(map(str,predict_attr[i]))) for i in range(len(predict_classid))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
